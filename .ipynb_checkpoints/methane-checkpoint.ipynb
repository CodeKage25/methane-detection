{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/Downloads/methane-detection/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-08-21 22:34:02.465305: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ UMAP available\n",
      "✅ TensorFlow available\n",
      "📦 All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import warnings\n",
    "import time\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "from io import BytesIO, StringIO\n",
    "\n",
    "# ML imports\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier, \n",
    "                             ExtraTreesClassifier, AdaBoostClassifier, VotingClassifier)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (accuracy_score, classification_report, roc_auc_score, \n",
    "                           roc_curve, auc, confusion_matrix, precision_recall_curve,\n",
    "                           f1_score, precision_score, recall_score)\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE, VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Optional imports\n",
    "try:\n",
    "    import umap\n",
    "    UMAP_AVAILABLE = True\n",
    "    print(\"✅ UMAP available\")\n",
    "except ImportError:\n",
    "    UMAP_AVAILABLE = False\n",
    "    print(\"⚠️ UMAP not available\")\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers, models, callbacks\n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "    from tensorflow.keras.applications import EfficientNetB0\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "    TF_AVAILABLE = True\n",
    "    print(\"✅ TensorFlow available\")\n",
    "except ImportError:\n",
    "    TF_AVAILABLE = False\n",
    "    print(\"⚠️ TensorFlow not available\")\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"📦 All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Output directories created in: output\n",
      "📊 Looking for data in: pipeline\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    \"\"\"Enhanced configuration with all settings\"\"\"\n",
    "    \n",
    "    # Base paths - Local data path\n",
    "    BASE_PATH = Path('./pipeline')  # Local path for VSCode\n",
    "    OUTPUT_BASE = Path('./output')\n",
    "    \n",
    "    HOLE_SIZES = ['20mm', '25mm', '30mm', '40mm']\n",
    "    \n",
    "    # Data formats\n",
    "    EXPECTED_COLUMNS = ['nodenumber', 'x-coordinate', 'y-coordinate', 'temperature']\n",
    "    \n",
    "    # Image processing\n",
    "    IMG_HEIGHT = 224\n",
    "    IMG_WIDTH = 224\n",
    "    IMG_CHANNELS = 3\n",
    "    SUPPORTED_IMG_FORMATS = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']\n",
    "    \n",
    "    # Model parameters\n",
    "    TEST_SIZE = 0.2\n",
    "    VAL_SIZE = 0.2\n",
    "    RANDOM_STATE = 42\n",
    "    \n",
    "    # Training parameters\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 50  # Reduced for faster training\n",
    "    LEARNING_RATE = 0.001\n",
    "    PATIENCE = 10\n",
    "    \n",
    "    # Feature engineering parameters\n",
    "    TEMPERATURE_THRESHOLD = 310  # Kelvin\n",
    "    SPATIAL_ZONES = 20\n",
    "    ROLLING_WINDOW = 5\n",
    "    \n",
    "    # Output paths\n",
    "    OUTPUT_DIR = OUTPUT_BASE\n",
    "    MODEL_DIR = OUTPUT_DIR / 'models'\n",
    "    PLOTS_DIR = OUTPUT_DIR / 'plots'\n",
    "    DATA_DIR = OUTPUT_DIR / 'processed_data'\n",
    "\n",
    "# Create output directories\n",
    "def setup_directories():\n",
    "    \"\"\"Setup all required directories\"\"\"\n",
    "    for dir_path in [Config.OUTPUT_DIR, Config.MODEL_DIR, Config.PLOTS_DIR, Config.DATA_DIR]:\n",
    "        dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    return True\n",
    "\n",
    "setup_directories()\n",
    "print(f\"📁 Output directories created in: {Config.OUTPUT_DIR}\")\n",
    "print(f\"📊 Looking for data in: {Config.BASE_PATH}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 📊 Enhanced Data Loader\n",
    "\n",
    "# %%\n",
    "class EnhancedDataLoader:\n",
    "    \"\"\"Enhanced data loader with pattern recognition\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def discover_data_files():\n",
    "        \"\"\"Discover all data files with pattern recognition\"\"\"\n",
    "        print(\"🔍 Discovering data files...\")\n",
    "        discovered_files = {}\n",
    "        \n",
    "        for hole_size in Config.HOLE_SIZES:\n",
    "            hole_path = Config.BASE_PATH / hole_size\n",
    "            discovered_files[hole_size] = {\n",
    "                'data_files': [],\n",
    "                'image_files': [],\n",
    "                'file_count': 0,\n",
    "                'image_count': 0\n",
    "            }\n",
    "            \n",
    "            if hole_path.exists():\n",
    "                print(f\"  📂 Found directory: {hole_path}\")\n",
    "                # Find data files\n",
    "                all_files = list(hole_path.iterdir())\n",
    "                data_files = []\n",
    "                \n",
    "                for file in all_files:\n",
    "                    if file.is_file():\n",
    "                        file_name = file.name.lower()\n",
    "                        \n",
    "                        # Skip hidden files and known non-data files\n",
    "                        if file_name.startswith('.'):\n",
    "                            continue\n",
    "                            \n",
    "                        # Check if it's likely a data file\n",
    "                        if (hole_size.replace('mm', '') in file_name or \n",
    "                            'test' in file_name or \n",
    "                            any(char.isdigit() for char in file_name)):\n",
    "                            data_files.append(file)\n",
    "                \n",
    "                discovered_files[hole_size]['data_files'] = data_files\n",
    "                discovered_files[hole_size]['file_count'] = len(data_files)\n",
    "                print(f\"    📄 Found {len(data_files)} data files\")\n",
    "                \n",
    "                # Find image files\n",
    "                contours_path = hole_path / 'contours'\n",
    "                if contours_path.exists():\n",
    "                    image_files = []\n",
    "                    for ext in Config.SUPPORTED_IMG_FORMATS:\n",
    "                        image_files.extend(list(contours_path.glob(f'*{ext}')))\n",
    "                        image_files.extend(list(contours_path.glob(f'*{ext.upper()}')))\n",
    "                    \n",
    "                    discovered_files[hole_size]['image_files'] = image_files\n",
    "                    discovered_files[hole_size]['image_count'] = len(image_files)\n",
    "                    print(f\"    🖼️ Found {len(image_files)} image files\")\n",
    "        \n",
    "        total_files = sum([data['file_count'] for data in discovered_files.values()])\n",
    "        total_images = sum([data['image_count'] for data in discovered_files.values()])\n",
    "        print(f\"\\n✅ Discovery complete: {total_files} data files, {total_images} images\")\n",
    "        \n",
    "        return discovered_files\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_data_file(file_path: Path) -> pd.DataFrame:\n",
    "        \"\"\"Parse data file with advanced format detection\"\"\"\n",
    "        try:\n",
    "            # Try different parsing methods\n",
    "            parsing_methods = [\n",
    "                lambda: pd.read_csv(file_path, sep=',', header=None),\n",
    "                lambda: pd.read_csv(file_path, sep='\\s+', header=None),\n",
    "                lambda: pd.read_csv(file_path, sep='\\t', header=None),\n",
    "                lambda: pd.read_csv(file_path, sep=None, engine='python', header=None),\n",
    "            ]\n",
    "            \n",
    "            df = None\n",
    "            for method in parsing_methods:\n",
    "                try:\n",
    "                    df = method()\n",
    "                    if len(df.columns) >= 4 and len(df) > 0:\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if df is None or df.empty:\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Assign column names\n",
    "            if len(df.columns) >= 4:\n",
    "                df.columns = Config.EXPECTED_COLUMNS[:len(df.columns)]\n",
    "                \n",
    "                if len(df.columns) > 4:\n",
    "                    additional_cols = [f'feature_{i}' for i in range(4, len(df.columns))]\n",
    "                    df.columns = Config.EXPECTED_COLUMNS + additional_cols\n",
    "            \n",
    "            # Data cleaning\n",
    "            df = df.dropna()\n",
    "            \n",
    "            # Ensure numeric columns\n",
    "            numeric_columns = ['x-coordinate', 'y-coordinate', 'temperature']\n",
    "            for col in numeric_columns:\n",
    "                if col in df.columns:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            df = df.dropna()\n",
    "            \n",
    "            # Validate temperature values\n",
    "            if 'temperature' in df.columns:\n",
    "                df = df[(df['temperature'] > 200) & (df['temperature'] < 1000)]\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not parse {file_path.name}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    @staticmethod\n",
    "    def advanced_leak_classification(df: pd.DataFrame, hole_size: str) -> Dict:\n",
    "        \"\"\"Advanced leak classification with multiple criteria\"\"\"\n",
    "        if df.empty or 'temperature' not in df.columns:\n",
    "            return {'leak_status': 0, 'confidence': 0.0, 'criteria_met': []}\n",
    "        \n",
    "        temp_stats = {\n",
    "            'mean': df['temperature'].mean(),\n",
    "            'max': df['temperature'].max(),\n",
    "            'std': df['temperature'].std(),\n",
    "            'q95': df['temperature'].quantile(0.95),\n",
    "        }\n",
    "        \n",
    "        # Hole size factors\n",
    "        hole_factors = {'20mm': 1.0, '25mm': 1.2, '30mm': 1.4, '40mm': 1.6}\n",
    "        factor = hole_factors.get(hole_size, 1.0)\n",
    "        \n",
    "        # Classification criteria\n",
    "        criteria = {}\n",
    "        criteria_met = []\n",
    "        \n",
    "        # Criterion 1: Mean temperature\n",
    "        temp_threshold = Config.TEMPERATURE_THRESHOLD * factor\n",
    "        criteria['mean_temp'] = temp_stats['mean'] > temp_threshold\n",
    "        if criteria['mean_temp']:\n",
    "            criteria_met.append(f\"Mean temp ({temp_stats['mean']:.1f}K) > {temp_threshold:.1f}K\")\n",
    "        \n",
    "        # Criterion 2: Maximum temperature\n",
    "        max_threshold = (Config.TEMPERATURE_THRESHOLD + 30) * factor\n",
    "        criteria['max_temp'] = temp_stats['max'] > max_threshold\n",
    "        if criteria['max_temp']:\n",
    "            criteria_met.append(f\"Max temp ({temp_stats['max']:.1f}K) > {max_threshold:.1f}K\")\n",
    "        \n",
    "        # Criterion 3: Temperature variability\n",
    "        std_threshold = 15 * factor\n",
    "        criteria['temp_variance'] = temp_stats['std'] > std_threshold\n",
    "        if criteria['temp_variance']:\n",
    "            criteria_met.append(f\"Temp std ({temp_stats['std']:.1f}) > {std_threshold:.1f}\")\n",
    "        \n",
    "        # Decision logic\n",
    "        criteria_count = sum(criteria.values())\n",
    "        total_criteria = len(criteria)\n",
    "        \n",
    "        is_leak = criteria_count >= (total_criteria // 2 + 1)\n",
    "        confidence = criteria_count / total_criteria\n",
    "        \n",
    "        return {\n",
    "            'leak_status': int(is_leak),\n",
    "            'confidence': confidence,\n",
    "            'criteria_met': criteria_met,\n",
    "            'criteria_count': criteria_count\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Discovering data files...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m EnhancedDataLoader()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Discover data files\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m discovered_files \u001b[38;5;241m=\u001b[39m \u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscover_data_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load all data\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m📊 Loading and processing data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m, in \u001b[0;36mEnhancedDataLoader.discover_data_files\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔍 Discovering data files...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m discovered_files \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hole_size \u001b[38;5;129;01min\u001b[39;00m \u001b[43mConfig\u001b[49m\u001b[38;5;241m.\u001b[39mHOLE_SIZES:\n\u001b[1;32m     11\u001b[0m     hole_path \u001b[38;5;241m=\u001b[39m Config\u001b[38;5;241m.\u001b[39mBASE_PATH \u001b[38;5;241m/\u001b[39m hole_size\n\u001b[1;32m     12\u001b[0m     discovered_files[hole_size] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_files\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_files\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_count\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_count\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     17\u001b[0m     }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Config' is not defined"
     ]
    }
   ],
   "source": [
    "data_loader = EnhancedDataLoader()\n",
    "\n",
    "# Discover data files\n",
    "discovered_files = data_loader.discover_data_files()\n",
    "\n",
    "# Load all data\n",
    "print(\"\\n📊 Loading and processing data...\")\n",
    "all_dataframes = []\n",
    "\n",
    "total_files = sum([data['file_count'] for data in discovered_files.values()])\n",
    "\n",
    "if total_files == 0:\n",
    "    print(\"❌ No data files found! Please ensure data is in './data/' directory\")\n",
    "    print(\"Expected structure:\")\n",
    "    print(\"./data/20mm/\")\n",
    "    print(\"./data/25mm/\")\n",
    "    print(\"./data/30mm/\")\n",
    "    print(\"./data/40mm/\")\n",
    "else:\n",
    "    file_count = 0\n",
    "    for hole_size, file_data in discovered_files.items():\n",
    "        if file_data['file_count'] == 0:\n",
    "            continue\n",
    "        \n",
    "        print(f\"  Processing {hole_size} files...\")\n",
    "        for file_path in file_data['data_files']:\n",
    "            df = data_loader.parse_data_file(file_path)\n",
    "            if not df.empty:\n",
    "                # Classify leaks\n",
    "                leak_info = data_loader.advanced_leak_classification(df, hole_size)\n",
    "                df['leak_status'] = leak_info['leak_status']\n",
    "                df['leak_confidence'] = leak_info['confidence']\n",
    "                df['hole_size'] = hole_size\n",
    "                df['file_name'] = file_path.name\n",
    "                all_dataframes.append(df)\n",
    "                \n",
    "                print(f\"    ✅ {file_path.name}: {len(df)} samples, Leak: {'Yes' if leak_info['leak_status'] else 'No'}\")\n",
    "            \n",
    "            file_count += 1\n",
    "    \n",
    "    if all_dataframes:\n",
    "        combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "        print(f\"\\n🎉 Successfully loaded {len(combined_df):,} total data points from {len(all_dataframes)} files\")\n",
    "        \n",
    "        # Display summary statistics\n",
    "        print(\"\\n📈 Dataset Summary:\")\n",
    "        print(f\"  Total samples: {len(combined_df):,}\")\n",
    "        print(f\"  Features: {len(combined_df.columns)}\")\n",
    "        print(f\"  Leak detections: {combined_df['leak_status'].sum():,}\")\n",
    "        print(f\"  No leak: {(combined_df['leak_status'] == 0).sum():,}\")\n",
    "        print(f\"  Leak rate: {combined_df['leak_status'].mean():.1%}\")\n",
    "        \n",
    "        # Show data info\n",
    "        print(\"\\n📋 Data Info:\")\n",
    "        combined_df.info()\n",
    "        \n",
    "        # Show first few rows\n",
    "        print(\"\\n👀 First 5 rows:\")\n",
    "        display(combined_df.head())\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ No data could be loaded successfully!\")\n",
    "        combined_df = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
